{"cells":[{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n","\n","<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Objective\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3> \n","\n","1.  <a href=\"https://#item31\">Import Libraries and Packages</a>\n","2.  <a href=\"https://#item32\">Download Data</a>\n","3.  <a href=\"https://#item33\">Define Global Constants</a>\n","4.  <a href=\"https://#item34\">Construct ImageDataGenerator Instances</a>\n","5.  <a href=\"https://#item35\">Compile and Fit Model</a>\n","\n","</font>\n","\n","</div>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":[]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item31'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Let's start the lab by importing the libraries that we will be using in this lab.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.applications.resnet50 import preprocess_input\n","#from keras.applications import ResNet50\n","#from keras.applications.resnet50 import preprocess_input"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item32'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Download Data\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["## get the data\n","#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"scrolled":true},"outputs":[],"source":["!unzip concrete_data_week3.zip"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50**\\* error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item33'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Here, we will define constants that we will be using throughout the rest of the lab.\n","\n","1.  We are obviously dealing with two classes, so *num_classes* is 2.\n","2.  The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3.  We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["num_classes = 2\n","\n","image_resize = 224\n","\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item34'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n",")"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":6,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 30001 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/train',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 10001 images belonging to 2 classes.\n"]}],"source":["## Type your answer here\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Double-click **here** for the solution.\n","\n","<!-- The correct answer is:\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","-->\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item35'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":8,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","94773248/94765736 [==============================] - 119s 1us/step\n","94781440/94765736 [==============================] - 119s 1us/step\n"]}],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["You can access the model's layers using the *layers* attribute of our model object.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"data":{"text/plain":["[<keras.engine.functional.Functional at 0x22e21fd3070>,\n"," <keras.layers.core.Dense at 0x22e0f6fcd30>]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model.layers"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":14,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"scrolled":true},"outputs":[{"data":{"text/plain":["[<keras.engine.input_layer.InputLayer at 0x22e199d0e50>,\n"," <keras.layers.convolutional.ZeroPadding2D at 0x22e199dd610>,\n"," <keras.layers.convolutional.Conv2D at 0x22e199ddd90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19a70ee0>,\n"," <keras.layers.core.Activation at 0x22e19a65df0>,\n"," <keras.layers.convolutional.ZeroPadding2D at 0x22e19ce8df0>,\n"," <keras.layers.pooling.MaxPooling2D at 0x22e19d1ff10>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19ce8700>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d3b940>,\n"," <keras.layers.core.Activation at 0x22e19d18f10>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d53bb0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d45550>,\n"," <keras.layers.core.Activation at 0x22e19d4d4c0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d28760>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d5f880>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d31580>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d6b8b0>,\n"," <keras.layers.merge.Add at 0x22e19d58550>,\n"," <keras.layers.core.Activation at 0x22e19d80730>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d7ce50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d4d400>,\n"," <keras.layers.core.Activation at 0x22e19d53b80>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d367f0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d18610>,\n"," <keras.layers.core.Activation at 0x22e19d72fa0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d89d00>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d86ee0>,\n"," <keras.layers.merge.Add at 0x22e19d896d0>,\n"," <keras.layers.core.Activation at 0x22e19d9b610>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d94d30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19da0df0>,\n"," <keras.layers.core.Activation at 0x22e19d94f40>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d9bb80>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d9bac0>,\n"," <keras.layers.core.Activation at 0x22e19da7d60>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19dc4970>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19dbfc70>,\n"," <keras.layers.merge.Add at 0x22e19dcc820>,\n"," <keras.layers.core.Activation at 0x22e19dd8ee0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19df2340>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19dff5e0>,\n"," <keras.layers.core.Activation at 0x22e19dffeb0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19df2280>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19dd8d90>,\n"," <keras.layers.core.Activation at 0x22e19df2310>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19de88b0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19dab610>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19de85b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19d86280>,\n"," <keras.layers.merge.Add at 0x22e19d94610>,\n"," <keras.layers.core.Activation at 0x22e19d45d90>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19db6dc0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19e05e50>,\n"," <keras.layers.core.Activation at 0x22e19db6340>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19e05a90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19e0c400>,\n"," <keras.layers.core.Activation at 0x22e19e05dc0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a2479a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1a23eca0>,\n"," <keras.layers.merge.Add at 0x22e1a24b850>,\n"," <keras.layers.core.Activation at 0x22e1a258f10>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a264130>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1a264610>,\n"," <keras.layers.core.Activation at 0x22e1a261fa0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a2727c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1a27d4c0>,\n"," <keras.layers.core.Activation at 0x22e1a2863d0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a2819a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1a27d5e0>,\n"," <keras.layers.merge.Add at 0x22e1a2728e0>,\n"," <keras.layers.core.Activation at 0x22e1a2644c0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d6b4f0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19db6c40>,\n"," <keras.layers.core.Activation at 0x22e1a286f40>,\n"," <keras.layers.convolutional.Conv2D at 0x22e19d8f880>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e19dab2b0>,\n"," <keras.layers.core.Activation at 0x22e19d86e50>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a29da90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1a294f70>,\n"," <keras.layers.merge.Add at 0x22e1a2a93d0>,\n"," <keras.layers.core.Activation at 0x22e1a29d7f0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1aaccc10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1aad6a00>,\n"," <keras.layers.core.Activation at 0x22e1a2940d0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1aaed340>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1aaf0e20>,\n"," <keras.layers.core.Activation at 0x22e1aae6af0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1aac53d0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb15190>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1aac5160>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb26c10>,\n"," <keras.layers.merge.Add at 0x22e1bb26e50>,\n"," <keras.layers.core.Activation at 0x22e1aaff880>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb1bcd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1aadd5e0>,\n"," <keras.layers.core.Activation at 0x22e19df2670>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a2ada90>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1a2925e0>,\n"," <keras.layers.core.Activation at 0x22e1a2a29d0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb2bac0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb2f970>,\n"," <keras.layers.merge.Add at 0x22e1a234670>,\n"," <keras.layers.core.Activation at 0x22e1bb35fa0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb3c4c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb3f850>,\n"," <keras.layers.core.Activation at 0x22e1bb3ceb0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb3f670>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb52be0>,\n"," <keras.layers.core.Activation at 0x22e1bb44910>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb5ea60>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb68b80>,\n"," <keras.layers.merge.Add at 0x22e1bb58b20>,\n"," <keras.layers.core.Activation at 0x22e1bb78fa0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb826a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb86970>,\n"," <keras.layers.core.Activation at 0x22e1bb86250>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb824f0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb68880>,\n"," <keras.layers.core.Activation at 0x22e1bb8d970>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a239be0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1a2929d0>,\n"," <keras.layers.merge.Add at 0x22e1bb440a0>,\n"," <keras.layers.core.Activation at 0x22e1bb35ee0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a2942b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb9b430>,\n"," <keras.layers.core.Activation at 0x22e1bb9bdf0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bb9edf0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bb92ee0>,\n"," <keras.layers.core.Activation at 0x22e1bb9e8e0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1bbb78b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1bbb0dc0>,\n"," <keras.layers.merge.Add at 0x22e1bbb0d60>,\n"," <keras.layers.core.Activation at 0x22e1dd01670>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1dd066a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1dd06e50>,\n"," <keras.layers.core.Activation at 0x22e1bbb7c10>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1dd06640>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1dd13400>,\n"," <keras.layers.core.Activation at 0x22e1dd25220>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1dd2c820>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1dd13430>,\n"," <keras.layers.merge.Add at 0x22e1dd0f7f0>,\n"," <keras.layers.core.Activation at 0x22e1bbb7e50>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1a234970>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1dd30eb0>,\n"," <keras.layers.core.Activation at 0x22e1bbbcf10>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1dd34af0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1dd345e0>,\n"," <keras.layers.core.Activation at 0x22e1dd355b0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e1aaf8c40>,\n"," <keras.layers.convolutional.Conv2D at 0x22e21f6ac40>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1dd1f0a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e21f6e130>,\n"," <keras.layers.merge.Add at 0x22e21f6e2b0>,\n"," <keras.layers.core.Activation at 0x22e21f882b0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e21f885e0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e21f90460>,\n"," <keras.layers.core.Activation at 0x22e21f90070>,\n"," <keras.layers.convolutional.Conv2D at 0x22e21f88fa0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e21f96670>,\n"," <keras.layers.core.Activation at 0x22e21f93df0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e21fadcd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e21fc1310>,\n"," <keras.layers.merge.Add at 0x22e21fc19d0>,\n"," <keras.layers.core.Activation at 0x22e21f93730>,\n"," <keras.layers.convolutional.Conv2D at 0x22e21fa0040>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e21f88d60>,\n"," <keras.layers.core.Activation at 0x22e21f6e8b0>,\n"," <keras.layers.convolutional.Conv2D at 0x22e21fa01c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e1dd320d0>,\n"," <keras.layers.core.Activation at 0x22e21f7b790>,\n"," <keras.layers.convolutional.Conv2D at 0x22e21fc7a30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x22e21fca3a0>,\n"," <keras.layers.merge.Add at 0x22e21fc8bb0>,\n"," <keras.layers.core.Activation at 0x22e21fcc400>,\n"," <keras.layers.pooling.GlobalAveragePooling2D at 0x22e21fd6940>]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","resnet50 (Functional)        (None, 2048)              23587712  \n","_________________________________________________________________\n","dense (Dense)                (None, 2)                 4098      \n","=================================================================\n","Total params: 23,591,810\n","Trainable params: 4,098\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":18,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["h:\\Coding_installation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  warnings.warn('`Model.fit_generator` is deprecated and '\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","301/301 [==============================] - 1236s 4s/step - loss: 0.0302 - accuracy: 0.9908 - val_loss: 0.0073 - val_accuracy: 0.9983\n","Epoch 2/2\n","301/301 [==============================] - 1193s 4s/step - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0049 - val_accuracy: 0.9993\n"]}],"source":["fit_history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=steps_per_epoch_training,\n","    epochs=num_epochs,\n","    validation_data=validation_generator,\n","    validation_steps=steps_per_epoch_validation,\n","    verbose=1,\n",")"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["h:\\Coding_installation\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  warnings.warn('Custom mask layers require a config and must override '\n"]}],"source":["model.save('classifier_resnet_model.h5')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3\\_LAB1).\n"]},{"cell_type":"markdown","metadata":{},"source":["## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n","| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n","| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<hr>\n","\n","Copyright © 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01).\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('tensorflow')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"d9a2c21cf3c70c1db3b4d9aafc4f872e6a19fefff8b66e180dee433f787d10e4"}}},"nbformat":4,"nbformat_minor":4}
